<p><small><em>总字数：约12000字，阅读时间：约15分钟</em></small></p>
<h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>刚开始接触机器学习的时候，曾经试过用python写过一个简单的MLP（Multi-Layer Perceptron），其中仅仅只是实现了简单的Gradient Descent算法，激活函数上也只涉及了Sigmod激活函数。在这之后的很多研究中，都是用的现成的算法库在跑，完全接触不到实现方法，编程能力也急剧下降。所以趁着现在有点时间，认真学习一下别人是怎么样实现深度学习方法的。由于我一直使用的是python语言，而且我在入门深度学习的时候也是从Keras这个库开始的。自从被TensorFlow列为官网API之后，Keras使用的人也极具上升。所以决定从Keras源码开始看起，学习一下别人是如何实现深度学习框架的。</p>
<p>由于我是即兴记录的，目前还没有任何的目标。所以可能会出现结构上有些凌乱，等我都掌握了之后，我再做整理吧。如果发现了我在分析的过程中存在错误或者不足，希望大家能给我留言，欢迎指正。</p>
<p>在发表之前写了一些内容，但是自低向上写的。结果发现自低向上分析实在有点困难，要直接把底层结构讲清楚确实很困难。所以决定改一改策略，采用自顶向下的方式写，先分析基础构建，再深入底层分析运作原理。注：在代码讲解过程中，我将一些不必要的空行，包导入等不影响整体介绍的部分删掉了，将非重点讲解的内容修改为pass(并不是这部分没有代码)，在部分位置添加了一些标识以定位代码。</p>
<p>我在开始写这个系列的时候，Keras最新版本还是1.2.2，现在已经出了2.X的版本了，不过核心框架没有变动，仅仅只是做了一些接口的改变，所以依旧可以根据1.2.2版本进行分析。Keras 2改动部分可以在这篇<a href="https://blog.keras.io/introducing-keras-2.html" target="_blank" rel="external">博文(Introducing Keras 2)</a>上查看。</p>
<a id="more"></a>
<h4 id="文档结构"><a href="#文档结构" class="headerlink" title="文档结构"></a>文档结构</h4><p>我们先来看看Keras下的文档结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">|-- docs               #说明文档</div><div class="line">|-- examples           #应用示例</div><div class="line">|-- test               #测试文件</div><div class="line">|-- keras                  #源码核心</div><div class="line">|  |-- backend             #底层backend</div><div class="line">|  |-- datasets            #数据获取源码</div><div class="line">|  |-- engine              #核心工具</div><div class="line">|  |-- layers              #层源码</div><div class="line">|  |-- legacy              #遗留源码</div><div class="line">|  |-- preprocessing       #预处理函数</div><div class="line">|  |-- utils               #实用工具</div><div class="line">|  |-- wrappers            #scikit-learn 封装类</div><div class="line">|  |-- activations.py      #可用的激活函数</div><div class="line">|  |-- callbacks.py        #回调函数</div><div class="line">|  |-- constraints.py      #权重约束项，如非零约束等</div><div class="line">|  |-- initializations.py  #初始化方法</div><div class="line">|  |-- metrics.py          #度量方法</div><div class="line">|  |-- models.py           #包含Model和Sequential模型，以及各种存取方法</div><div class="line">|  |-- objectives.py       #objectives function，也就是loss function</div><div class="line">|  |-- optimizers.py       #优化方法，如SGD，Adam等</div><div class="line">|  |-- regularizers.py     #规则项，如L1，L2等规则项</div></pre></td></tr></table></figure>
<p>实际上，对于在深度学习这个模块里面，我们主要关注的是engine文件夹中的内容，这里面包含了关于model、layer、node的实现方式，也就是最底层的内容。而我们在应用当中，主要使用的Keras文件夹下.py的文件，其中常用的Sequential模型在models.py中实现，各种Optimizer类在optimizers.py中实现，各种激活函数在activations.py等等。这些内容我们以后慢慢来看，我们先从底层开始看起，看看Keras的底层是如何实现可扩展结构的。</p>
<h4 id="从一个例子开始"><a href="#从一个例子开始" class="headerlink" title="从一个例子开始"></a>从一个例子开始</h4><p>首先，从一个示例程序入手，开始解析Keras。我从examples文件夹中挑选mnist_cnn<br>.py作为我们上手的第一个程序。选择它的主要原因是因为它整体框架比较简洁，而且包含了Keras常用的构建。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"># examples/mnist_cnn.py</div><div class="line">&apos;&apos;&apos;Trains a simple convnet on the MNIST dataset.</div><div class="line">Gets to 99.25% test accuracy after 12 epochs</div><div class="line">(there is still a lot of margin for parameter tuning).</div><div class="line">16 seconds per epoch on a GRID K520 GPU.</div><div class="line">&apos;&apos;&apos;</div><div class="line">batch_size = 128</div><div class="line">nb_classes = 10</div><div class="line">nb_epoch = 12</div><div class="line"></div><div class="line"># input image dimensions</div><div class="line">img_rows, img_cols = 28, 28</div><div class="line"># number of convolutional filters to use</div><div class="line">nb_filters = 32</div><div class="line"># size of pooling area for max pooling</div><div class="line">pool_size = (2, 2)</div><div class="line"># convolution kernel size</div><div class="line">kernel_size = (3, 3)</div><div class="line"></div><div class="line"># the data, shuffled and split between train and test sets</div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line"></div><div class="line">if K.image_dim_ordering() == &apos;th&apos;:</div><div class="line">    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)</div><div class="line">    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)</div><div class="line">    input_shape = (1, img_rows, img_cols)</div><div class="line">else:</div><div class="line">    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)</div><div class="line">    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)</div><div class="line">    input_shape = (img_rows, img_cols, 1)</div><div class="line"></div><div class="line">X_train = X_train.astype(&apos;float32&apos;)</div><div class="line">X_test = X_test.astype(&apos;float32&apos;)</div><div class="line">X_train /= 255</div><div class="line">X_test /= 255</div><div class="line">print(&apos;X_train shape:&apos;, X_train.shape)</div><div class="line">print(X_train.shape[0], &apos;train samples&apos;)</div><div class="line">print(X_test.shape[0], &apos;test samples&apos;)</div><div class="line"></div><div class="line"># convert class vectors to binary class matrices</div><div class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</div><div class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</div><div class="line">model = Sequential()</div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],</div><div class="line">                        border_mode=&apos;valid&apos;,</div><div class="line">                        input_shape=input_shape))</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">model.add(MaxPooling2D(pool_size=pool_size))</div><div class="line">model.add(Dropout(0.25))</div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dense(128))</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">model.add(Dropout(0.5))</div><div class="line">model.add(Dense(nb_classes))</div><div class="line">model.add(Activation(&apos;softmax&apos;))</div><div class="line">model.compile(loss=&apos;categorical_crossentropy&apos;,</div><div class="line">              optimizer=&apos;adadelta&apos;,</div><div class="line">              metrics=[&apos;accuracy&apos;])</div><div class="line">model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,</div><div class="line">          verbose=1, validation_data=(X_test, Y_test))</div><div class="line">score = model.evaluate(X_test, Y_test, verbose=0)</div><div class="line">print(&apos;Test score:&apos;, score[0])</div><div class="line">print(&apos;Test accuracy:&apos;, score[1])</div></pre></td></tr></table></figure>
<p>首先定义了这个模型的一些超参数，batch size,input size之类的。其次是获取数据。这里我们需要重点提一下了，在Keras的datasets模块中，包含了6个数据集(cifar, cifar10, cifar100, imdb, mnist, reuters)，这些数据集不是一开始就存在的，而是在使用的时候才去下载的，不过只需要下载一次就可以了，之后它会在指定的文件夹中寻找。我们看看它是怎么实现的。</p>
<h4 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># keras/datasets/mnist.py</div><div class="line">def load_data(path=&apos;mnist.pkl.gz&apos;):</div><div class="line">    &quot;&quot;&quot;Loads the MNIST dataset.</div><div class="line">    # Arguments</div><div class="line">        path: path where to cache the dataset locally</div><div class="line">            (relative to ~/.keras/datasets).</div><div class="line">    # Returns</div><div class="line">        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    path = get_file(path, origin=&apos;https://s3.amazonaws.com/img-datasets/mnist.pkl.gz&apos;)</div><div class="line">    if path.endswith(&apos;.gz&apos;):</div><div class="line">        f = gzip.open(path, &apos;rb&apos;)</div><div class="line">    else:</div><div class="line">        f = open(path, &apos;rb&apos;)</div><div class="line">    if sys.version_info &lt; (3,):</div><div class="line">        data = cPickle.load(f)</div><div class="line">    else:</div><div class="line">        data = cPickle.load(f, encoding=&apos;bytes&apos;)</div><div class="line">    f.close()</div><div class="line">    return data  # (x_train, y_train), (x_test, y_test)</div></pre></td></tr></table></figure>
<p>load_data函数接受一个参数path，表示存储的地址。关于这个参数，我觉得这是Keras设计上的一个失误。无论是文件名还是注释都表示这是一个地址，但实际的默认给出的参数却是文件名，然后又在注释中又添加了一段(relative to ~/.keras/datasets)，表示实际的默认存储位置为用户目录下的一个隐藏文件中，确实让人很不解。继续往下看，从get_file的URL地址我们可以看出，Keras将数据文件寄托在了Amazon AWS上面。对于一些小一点的数据文件，直接用其自动下载功能不会存在太大的问题。但是对于一些比较大的文件，直接下载很容易断线，而默认是不支持断点续传的。这个时候我们就可以在源码里面找到它的下载URL，然后使用其他的下载工具进行下载，然后拷贝到默认的文件夹(~/.keras/datasets)中即可。</p>
<p>之后是检测是否为压缩包，若是则读取压缩文件，否则直接读取(可以看出这个应该是早期设计的函数，解压部分现在已经被集成到了<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">这里我们再深入一点，看看```get_file```函数的运作方式。这个函数很重要，Keras中涉及到下载文件功能的部分均是由这个函数实现的。它保存在utils\data_utils.py文件中。</div></pre></td></tr></table></figure></p>
<h1 id="keras-utils-data-utils-py"><a href="#keras-utils-data-utils-py" class="headerlink" title="keras/utils/data_utils.py"></a>keras/utils/data_utils.py</h1><p>def get_file(fname, origin, untar=False,<br>             md5_hash=None, cache_subdir=’datasets’):<br>    “””Downloads a file from a URL if it not already in the cache.<br>    Passing the MD5 hash will verify the file after download<br>    as well as if it is already present in the cache.</p>
<pre><code># Arguments
    fname: name of the file
    origin: original URL of the file
    untar: boolean, whether the file should be decompressed
    md5_hash: MD5 hash of the file for verification
    cache_subdir: directory being used as the cache
# Returns
    Path to the downloaded file
&quot;&quot;&quot;
######################## (1) start ########################
datadir_base = os.path.expanduser(os.path.join(&apos;~&apos;, &apos;.keras&apos;))
if not os.access(datadir_base, os.W_OK):
    datadir_base = os.path.join(&apos;/tmp&apos;, &apos;.keras&apos;)
datadir = os.path.join(datadir_base, cache_subdir)
if not os.path.exists(datadir):
    os.makedirs(datadir)
######################### (1) end #########################
if untar:
    untar_fpath = os.path.join(datadir, fname)
    fpath = untar_fpath + &apos;.tar.gz&apos;
else:
    fpath = os.path.join(datadir, fname)
######################## (2) start ########################
download = False
if os.path.exists(fpath):
    # File found; verify integrity if a hash was provided.
    if md5_hash is not None:
        if not validate_file(fpath, md5_hash):
            print(&apos;A local file was found, but it seems to be &apos;
                  &apos;incomplete or outdated.&apos;)
            download = True
else:
    download = True
if download:
    print(&apos;Downloading data from&apos;, origin)
    progbar = None

    def dl_progress(count, block_size, total_size, progbar=None):
        if progbar is None:
            progbar = Progbar(total_size)
        else:
            progbar.update(count * block_size)

    error_msg = &apos;URL fetch failure on {}: {} -- {}&apos;
    try:
        try:
            urlretrieve(origin, fpath,
                        functools.partial(dl_progress, progbar=progbar))
        except URLError as e:
            raise Exception(error_msg.format(origin, e.errno, e.reason))
        except HTTPError as e:
            raise Exception(error_msg.format(origin, e.code, e.msg))
    except (Exception, KeyboardInterrupt) as e:
        if os.path.exists(fpath):
            os.remove(fpath)
        raise
    progbar = None
######################### (2) end #########################
if untar:
    if not os.path.exists(untar_fpath):
        print(&apos;Untaring file...&apos;)
        tfile = tarfile.open(fpath, &apos;r:gz&apos;)
        try:
            tfile.extractall(path=datadir)
        except (Exception, KeyboardInterrupt) as e:
            if os.path.exists(untar_fpath):
                if os.path.isfile(untar_fpath):
                    os.remove(untar_fpath)
                else:
                    shutil.rmtree(untar_fpath)
            raise
        tfile.close()
    return untar_fpath
return fpath
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">这个函数包含五个参数，fname(文件名)，origin(源URL)，untar(是否解压)，md5_hash(md5验证)，cache_subdir(存储子文件夹)。这个函数首先获取一个合法的下载地址(1)，如果根目录下.keras文件可写，则以这个路径为默认路径，否则默认路径改为/tmp/.keras。这里貌似有个bug，第一部分使用```os.path.expanduser```能兼容Windows、Linux等多种平台，但是对于后面直接使用```os.path.join(&apos;/tmp&apos;, &apos;.keras&apos;)```，貌似在Windows系统下面是不存在这个路径的。</div><div class="line"></div><div class="line">其后检测是否需要解压，这个功能应该是后来加的，在```mnist.load_data```函数中还留有需要外部解压的痕迹。对于需要解压的情况，函数的最后面对其进行了一个解压操作，并返回解压后的地址。</div><div class="line"></div><div class="line">这个函数的核心部分就是下载(2)。首先定义一个布尔变量download(注意:这个变量表示是否需要下载，而非是否已经下载)，然后检验是否需要下载，两种情况需要下载：文件不存在或者文件存在但MD5验证不正确。下载的过程就比较简单了，先定义了一个用于显示下载进度条的回调函数```dl_progress```，然后调取```urlretrieve```函数进行下载。我很喜欢这一部分的异常处理，在内部截获两种不同的异常，然后使用通用异常进行显示，并且将用户终止异常添加进去，还处理了下载失败后的损坏文件，避免出错。不过这里还是存在一个问题，这里缺少了MD5验证流程。实际上对于一个严谨的程序，应当将验证流程放置在整个程序的最末尾，以防止中间过程出现的不可预见性错误。所以这里应该适当调整MD5验证的位置，以防止意外情况出现。</div><div class="line"></div><div class="line">最后小小的提一下```urlretrieve```函数，主要是觉得这种处理方式很优雅。</div></pre></td></tr></table></figure>
<h1 id="keras-utils-data-utils-py-1"><a href="#keras-utils-data-utils-py-1" class="headerlink" title="keras/utils/data_utils.py"></a>keras/utils/data_utils.py</h1><p>if sys.version_info[0] == 2:<br>    def urlretrieve(url, filename, reporthook=None, data=None):<br>        pass<br>else:<br>    from six.moves.urllib.request import urlretrieve<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">因为在Python 2.X的版本中，urlretrieve存在一些问题，所以对于这个版本作者自己写了一个获取程序，而对于Python 3.X就直接从外部库导入。在保持接口相同的情况下，就可以兼容两个版本，而且不需要额外的修改。</div><div class="line"></div><div class="line">#### 一些小工具</div><div class="line"></div><div class="line">回到上面的示例程序，获取完了数据之后，我们又可以开心的继续往下走了。下面是检验backend维度顺序，在Theano中默认维度顺序为(depth, row, column)，而在TensorFlow中为(row, column, depth)，所以这里要加以区分。之后就是做一些简单的数据预处理，这里直接跳过。这一小节重点要讲的是```to_categorical```函数，这个函数在Keras中经常使用到，用于做标签转换的。</div><div class="line"></div><div class="line">在神经网络中，当我们要计算交叉熵的时候，我们需要将输出节点个数定为类别数C。并且将每一个样本label的维度转换为C维，标签值对应位置设为1，其余为0。这个时候就需要用到```to_categorical```函数了。</div></pre></td></tr></table></figure></p>
<h1 id="keras-utils-np-utils-py"><a href="#keras-utils-np-utils-py" class="headerlink" title="keras/utils/np_utils.py"></a>keras/utils/np_utils.py</h1><p>def to_categorical(y, nb_classes=None):<br>    “””Converts a class vector (integers) to binary class matrix.<br>    E.g. for use with categorical_crossentropy.</p>
<pre><code># Arguments
    y: class vector to be converted into a matrix
        (integers from 0 to nb_classes).
    nb_classes: total number of classes.
# Returns
    A binary matrix representation of the input.
&quot;&quot;&quot;
y = np.array(y, dtype=&apos;int&apos;).ravel()
if not nb_classes:
    nb_classes = np.max(y) + 1
n = y.shape[0]
categorical = np.zeros((n, nb_classes))
categorical[np.arange(n), y] = 1
return categorical
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">实际上，这个函数的实现特别简单，就是获取类别数C，然后创建一个NxC维的零矩阵，然后将对应值设置为1。这里想提一下的是，它的类别数获取是根据标签值中的最大值加一来获取的，也就是说原始标签一定要从0开始取值。否则会出现未定义的类别标签。</div><div class="line"></div><div class="line">当然啦，这里还有一个反向操作的工具，```probas_to_classes```函数</div></pre></td></tr></table></figure>
<p>def probas_to_classes(y_pred):<br>    if len(y_pred.shape) &gt; 1 and y_pred.shape[1] &gt; 1:<br>        return categorical_probas_to_classes(y_pred)<br>    return np.array([1 if p &gt; 0.5 else 0 for p in y_pred])</p>
<p>def categorical_probas_to_classes(p):<br>    return np.argmax(p, axis=1)<br>```</p>
<p>实际上，<code>categorical_probas_to_classes</code>函数才是<code>to_categorical</code>函数的逆向操作，不过作者考虑到两类识别的兼容性问题，在其上又加了一个维度判断，所以我们直接用<code>probas_to_classes</code>函数就好了。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>本节主要讲了一下主体框架，然后以一个例子入手，分析了数据下载模块和两个实用的类别转换小工具。下一个章节将重点分析网络模型建立。</p>
